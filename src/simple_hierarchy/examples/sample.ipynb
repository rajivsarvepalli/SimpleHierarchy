{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RI_TtYxZ_MJC"
   },
   "source": [
    "# Getting Started with simple-hierarchy\n",
    "\n",
    "You can also view this notebook in [google colab](https://colab.research.google.com/drive/1wT63yQ4K-XcZRg5Oy-NCt8b4QBDoVE8i?usp=sharing).\n",
    "\n",
    "Below are two examples of how to use this library.\n",
    "\n",
    "Please read the [documentation](https://simplehierarchy.readthedocs.io/en/latest/) for more information or check the [PyPI](https://pypi.org/project/simple-hierarchy/) page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download  simple-hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H1Nd1Peg_Ufs",
    "outputId": "b93c1816-608b-4d85-b389-2d20d10c467d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simple-hierarchy in c:\\users\\rajiv sarvepalli\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy==1.19.3 in c:\\users\\rajiv sarvepalli\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from simple-hierarchy) (1.19.3)\n",
      "Requirement already satisfied: torch<2.0.0,>=1.7.1; python_version >= \"3.7\" and python_version < \"4.0\" in c:\\users\\rajiv sarvepalli\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from simple-hierarchy) (1.7.1+cu101)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\rajiv sarvepalli\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch<2.0.0,>=1.7.1; python_version >= \"3.7\" and python_version < \"4.0\"->simple-hierarchy) (3.7.4.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\rajiv sarvepalli\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install simple-hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Hierarchy \n",
    "Below is an image illustrating the class hierarchy. The letter represent the class names and the numbers are the number of classes for that specific class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "jHCxRL-ef9Zd",
    "outputId": "2852e85f-2247-47e6-8c3d-8f4c3eff0df3"
   },
   "outputs": [],
   "source": [
    "# Create a hierarchy\n",
    "hierarchy = {\n",
    "    (\"A\", 2) : [(\"B\", 5), (\"C\", 7)],\n",
    "    (\"H\", 2) : [(\"A\", 2), (\"K\", 7), (\"L\", 10)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation and Exploration\n",
    "We are going to create a model where the parent outputs are fowarded from the last layers to the second to last child layer. The second example will cover how to forward from a specific layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yw5jztdD_XIz"
   },
   "outputs": [],
   "source": [
    "from simple_hierarchy.hierarchal_model import HierarchalModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model_b = nn.ModuleList([nn.Linear(10, 10) for i in range(4)])\n",
    "model = HierarchalModel(\n",
    "    model=model_b, k=2, hierarchy=hierarchy, size=(10, 10, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1Ash67dzgdCH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "H 2 [A 2 [B 5 [], C 7 []], K 7 [], L 10 []]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the model's tree\n",
    "model.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-YC3q1mAgffa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (1): Linear(in_features=10, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base layers\n",
    "model.base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NlFBN12ugh-c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (('H', 2)): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (3): Linear(in_features=10, out_features=2, bias=True)\n",
       "  )\n",
       "  (('A', 2)): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=10, bias=True)\n",
       "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (3): Linear(in_features=10, out_features=2, bias=True)\n",
       "  )\n",
       "  (('B', 5)): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=10, bias=True)\n",
       "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (3): Linear(in_features=10, out_features=5, bias=True)\n",
       "  )\n",
       "  (('C', 7)): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=10, bias=True)\n",
       "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (3): Linear(in_features=10, out_features=7, bias=True)\n",
       "  )\n",
       "  (('K', 7)): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=10, bias=True)\n",
       "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (3): Linear(in_features=10, out_features=7, bias=True)\n",
       "  )\n",
       "  (('L', 10)): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=10, bias=True)\n",
       "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the layers that are distinct per class\n",
    "# the additional layers are to link together differing output sizes to the provided layers (two additional layers per class)\n",
    "# in a later version these may be customizable to the layers you want (should you want to proivde distinct aspects of non-linear layers; currently requires conversion into linear layers) \n",
    "model.last_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Training the model is just like training any other PyTorch model, and this model should be able to do everything that every other PyTorch model including things TensorBoard plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "The same hierarchy but with parents output being forwarded from specific index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rajiv sarvepalli\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class DemoModel(nn.Module):\n",
    "    def __init__(self, base_model, size, model_layers, k, feed_from):\n",
    "        super(DemoModel, self).__init__()\n",
    "        # Create a hierarchy\n",
    "        hierarchy = {\n",
    "            (\"A\", 2) : [(\"B\", 5), (\"C\", 7)],\n",
    "            (\"H\", 2) : [(\"A\", 2), (\"K\", 7), (\"L\", 10)]\n",
    "        }\n",
    "        self.model = HierarchalModel(\n",
    "                        base_model=base_model,\n",
    "                        hierarchy=hierarchy,\n",
    "                        size=size,\n",
    "                        model=model_layers,\n",
    "                        k=k,\n",
    "                        feed_from=feed_from,\n",
    "                    )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "base_model = torchvision.models.resnext101_32x8d(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the indepdent layers of parent and children \n",
    "model_layers = [\n",
    "    nn.Linear(800, 750),\n",
    "    nn.Linear(750, 512),\n",
    "    # the output of this layer is feed forward from parent to child\n",
    "    nn.Linear(512, 128),\n",
    "    nn.Linear(128, 64),\n",
    "]\n",
    "\n",
    "# 1000 is the output size of our base model (the resnext101_32x8d)\n",
    "# 800 is the input size of our additional indepdent layers (called model_layers)\n",
    "# 64 is the output size of our additional indepdent layers (called model_layers)\n",
    "# 128 is the output size of second to last additional indepdent layer to feed forward from parent to child (with concatenation)\n",
    "size = (1000,800,64,128)\n",
    "# all 4 layers are distinct for each grouping of classes of model_layers\n",
    "k = 4\n",
    "# we want to feed from the second to last layer (from parent to child (with concatenation))\n",
    "feed_from = 1\n",
    "model = DemoModel(base_model, size, model_layers, k, feed_from)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2])\n",
      "torch.Size([8, 2])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8, 7])\n",
      "torch.Size([8, 7])\n",
      "torch.Size([8, 10])\n"
     ]
    }
   ],
   "source": [
    "out = torch.rand((8, 3,512,512))\n",
    "pred = model(out)\n",
    "for p in pred:\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (('H', 2)): Sequential(\n",
       "    (0): Linear(in_features=1000, out_features=800, bias=True)\n",
       "    (1): Linear(in_features=800, out_features=750, bias=True)\n",
       "    (2): Linear(in_features=750, out_features=512, bias=True)\n",
       "    (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       "  (('A', 2)): Sequential(\n",
       "    (0): Linear(in_features=1128, out_features=800, bias=True)\n",
       "    (1): Linear(in_features=800, out_features=750, bias=True)\n",
       "    (2): Linear(in_features=750, out_features=512, bias=True)\n",
       "    (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       "  (('B', 5)): Sequential(\n",
       "    (0): Linear(in_features=1128, out_features=800, bias=True)\n",
       "    (1): Linear(in_features=800, out_features=750, bias=True)\n",
       "    (2): Linear(in_features=750, out_features=512, bias=True)\n",
       "    (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): Linear(in_features=64, out_features=5, bias=True)\n",
       "  )\n",
       "  (('C', 7)): Sequential(\n",
       "    (0): Linear(in_features=1128, out_features=800, bias=True)\n",
       "    (1): Linear(in_features=800, out_features=750, bias=True)\n",
       "    (2): Linear(in_features=750, out_features=512, bias=True)\n",
       "    (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): Linear(in_features=64, out_features=7, bias=True)\n",
       "  )\n",
       "  (('K', 7)): Sequential(\n",
       "    (0): Linear(in_features=1128, out_features=800, bias=True)\n",
       "    (1): Linear(in_features=800, out_features=750, bias=True)\n",
       "    (2): Linear(in_features=750, out_features=512, bias=True)\n",
       "    (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): Linear(in_features=64, out_features=7, bias=True)\n",
       "  )\n",
       "  (('L', 10)): Sequential(\n",
       "    (0): Linear(in_features=1128, out_features=800, bias=True)\n",
       "    (1): Linear(in_features=800, out_features=750, bias=True)\n",
       "    (2): Linear(in_features=750, out_features=512, bias=True)\n",
       "    (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.last_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can use a complex class hierarchy with a complex model in just a few lines of code. Additionally, this model includes feed forwarding from the parent class to the child class from a user-specified layer. In this case, the output of the Linear layer(256, 128) is feed-forward from each parent down into the child. The only independent layer (separate for every grouping of classes) is the Linear layer(128, 64) in this case, but there could be any number of layers as long as the parameters of `k` and `feed_from` are altered accordingly.\n",
    "\n",
    "The training can be done in the same manner as the first example. The model is just like any other PyTorch model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Class Hierarchy Models.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
